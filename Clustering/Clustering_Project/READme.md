# Semantic Task Clustering â€“ Training & Inference Pipeline

Ce projet implÃ©mente une **chaÃ®ne complÃ¨te de prÃ©paration, entraÃ®nement et infÃ©rence** pour le clustering sÃ©mantique de tÃ¢ches utilisateur, basÃ©e sur des embeddings Sentence Transformers et un raffinement hiÃ©rarchique contrÃ´lÃ© par des mÃ©triques de cohÃ©sion.

Lâ€™objectif est dâ€™obtenir des **clusters interprÃ©tables**, reprÃ©sentant des **tÃ¢ches globales cohÃ©rentes**, Ã©ventuellement composÃ©es de sous-tÃ¢ches, tout en acceptant lâ€™existence de tÃ¢ches autonomes (singletons).

---

## ğŸ“Œ Objectifs du projet

* Apprendre des reprÃ©sentations sÃ©mantiques de tÃ¢ches orientÃ©es *objectif global*
* Structurer les donnÃ©es sous forme hiÃ©rarchique (sous-tÃ¢ches â†’ tÃ¢che globale)
* EntraÃ®ner un modÃ¨le sans nÃ©gatifs explicites (contrastif implicite)
* Produire, Ã  lâ€™infÃ©rence, des clusters :

  * cohÃ©rents,
  * interprÃ©tables,
  * stables,
  * adaptÃ©s Ã  une intÃ©gration logicielle

---

## ğŸ§± Structure globale du pipeline

1. **Feature engineering & structuration des donnÃ©es**
2. **Construction des exemples dâ€™entraÃ®nement**
3. **EntraÃ®nement du modÃ¨le dâ€™embeddings**
4. **InfÃ©rence multi-Ã©tapes avec reclustering adaptatif**

---

## 1ï¸âƒ£ Feature Engineering & Structuration des donnÃ©es

### Normalisation des tÃ¢ches

Un nettoyage minimal est appliquÃ© afin de rÃ©duire le bruit textuel tout en conservant lâ€™information sÃ©mantique.

```python
def normalize_task(text):
    """
    Nettoyage minimal d'un task_item ou global_task
    """
    text = text.replace('\\"', '').replace('"', '')
    text = text.lower().strip()
    return text
```

---

### Structuration hiÃ©rarchique des donnÃ©es

Chaque entrÃ©e du dataset est transformÃ©e en une structure Ã  deux niveaux :

* **Niveau A** : sous-tÃ¢ches unitaires (`small_tasks`)
* **Niveau B** : tÃ¢che globale (`global_block`) obtenue par concatÃ©nation

```python
structured_dataset = []

for entry in dataset:
    task_items = entry["task_items"]

    small_tasks = [normalize_task(t) for t in task_items]
    global_block = " ".join(small_tasks)

    structured_dataset.append({
        "id": entry["id"],
        "small_tasks": small_tasks,
        "global_block": global_block,
        "global_task_description": normalize_task(entry["global_task_description"])
    })
```

ğŸ‘‰ Cette structuration permet dâ€™apprendre explicitement la relation *sous-tÃ¢che â†’ tÃ¢che globale*.

---

## 2ï¸âƒ£ Construction des exemples dâ€™entraÃ®nement

Les exemples sont construits sous forme de **paires positives uniquement**, compatibles avec `MultipleNegativesRankingLoss`.

Trois types de relations sont utilisÃ©s :

* sous-tÃ¢che â†” tÃ¢che globale
* sous-tÃ¢che â†” sous-tÃ¢che (cohÃ©sion intra-cluster)
* sous-ensemble partiel de sous-tÃ¢ches â†” tÃ¢che globale (data augmentation)

Les exemples sont ensuite sauvegardÃ©s pour rÃ©utilisation.

```python
with open(save_path, "wb") as f:
    pickle.dump(train_examples, f)
```

---

## 3ï¸âƒ£ EntraÃ®nement du modÃ¨le dâ€™embeddings

### ModÃ¨le de base

```python
model_name = "all-MiniLM-L6-v2"
model = SentenceTransformer(model_name)
```

### Dataloader & loss

```python
train_dataloader = DataLoader(
    train_examples,
    shuffle=True,
    batch_size=8
)

train_loss = losses.MultipleNegativesRankingLoss(model)
```

### EntraÃ®nement

```python
model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=3,
    warmup_steps=warmup_steps,
    show_progress_bar=True,
    checkpoint_path=CHECKPOINT_DIR,
    checkpoint_save_steps=2000,
    checkpoint_save_total_limit=3
)
```

* Pas de nÃ©gatifs explicites
* NÃ©gatifs implicites fournis par le batch
* Checkpoints automatiques
* ModÃ¨le final sauvegardÃ© pour lâ€™infÃ©rence

---

## 4ï¸âƒ£ InfÃ©rence & Clustering sÃ©mantique (vue dâ€™ensemble)

Lâ€™infÃ©rence repose sur un **pipeline en 4 Ã©tapes**, conÃ§u pour maximiser la cohÃ©rence et lâ€™interprÃ©tabilitÃ© des clusters.

### Step 1 â€” Clustering global initial

* Encodage des tÃ¢ches
* Matrice de distances cosinus
* Clustering hiÃ©rarchique agglomÃ©ratif
* SÃ©lection du meilleur seuil via silhouette

### Step 2 â€” Reclustering itÃ©ratif par cohÃ©sion

* Calcul de la cohÃ©sion intra-cluster
* Reclustering local des clusters trop hÃ©tÃ©rogÃ¨nes
* Gestion explicite des petits clusters

### Step 3 â€” Traitement des singletons

* Extraction des tÃ¢ches isolÃ©es
* Reclustering optionnel si leur proportion est significative
* Fusion contrÃ´lÃ©e avec les clusters existants

### Step 4 â€” Reclustering adaptatif final

* Analyse fine des clusters encore hÃ©tÃ©rogÃ¨nes
* Division rÃ©cursive jusquâ€™Ã  atteindre une cohÃ©sion acceptable
* Validation finale par silhouette et cohÃ©sion moyenne

ğŸ‘‰ Le rÃ©sultat final est un ensemble de clusters :

* centrÃ©s sur des tÃ¢ches globales,
* acceptant les singletons pertinents,
* directement exploitables dans un logiciel.

---

## ğŸ“Š MÃ©triques utilisÃ©es

* **Distance cosinus** (embeddings normalisÃ©s)
* **Silhouette score** (qualitÃ© globale)
* **CohÃ©sion intra-cluster moyenne** (qualitÃ© locale)

Les singletons sont volontairement exclus des mÃ©triques de cohÃ©sion.

---

## ğŸ§  Philosophie de conception

* Pas de nombre de clusters imposÃ©
* Pas de nÃ©gatifs explicites
* PrioritÃ© Ã  lâ€™interprÃ©tabilitÃ© sÃ©mantique
* Clusters = tÃ¢ches globales, pas simples similaritÃ©s lexicales
* Singletons autorisÃ©s sâ€™ils reprÃ©sentent une tÃ¢che autonome

---

## ğŸš€ IntÃ©gration logicielle

Ce pipeline est conÃ§u pour Ãªtre :

* embarquÃ© dans un logiciel dâ€™analyse dâ€™activitÃ©s utilisateur,
* utilisÃ© en batch ou en quasi temps rÃ©el,
* Ã©tendu avec de nouveaux seuils ou heuristiques mÃ©tier.

---

## ğŸ“ Artefacts produits

* `train_examples.pkl` : exemples dâ€™entraÃ®nement
* `checkpoints/` : modÃ¨les intermÃ©diaires
* `final_model/` : modÃ¨le final prÃªt pour lâ€™infÃ©rence

---

## âœ… Statut

âœ” Feature engineering
âœ” EntraÃ®nement
âœ” InfÃ©rence multi-Ã©tapes
âœ” PrÃªt pour intÃ©gration logicielle

